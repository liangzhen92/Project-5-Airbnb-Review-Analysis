{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim as gensimvis\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv('listings 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('df_final.pkl','rb') as read_file:\n",
    "    df_final = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_final.iloc[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "review_sent = []\n",
    "for i in reviews:\n",
    "    review_sent += sent_tokenize(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sent = set(review_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1153359"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "df_list = list(sent_to_words(review_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaoze/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(df_list, min_count=20) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[df_list], min_count=10)  \n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = [trigram_model[bigram_model[t]] for t in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# NOUN, ADJ, VERB, ADV\n",
    "def lemmatization(texts, allowed_postags=['NOUN']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "#         print(sent)\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "#         print(doc)\n",
    "        output_text = [token.lemma_ for token in doc if token.pos_ in allowed_postags and token.is_stop == False]\n",
    "#         print(output_text)\n",
    "        if len(output_text) > 0:\n",
    "            texts_out.append(output_text)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = lemmatization(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_sents_adjusted.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_sents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transport connection want \n",
      "\n",
      "home gem \n",
      "\n",
      "bus pad \n",
      "\n",
      "location tenderloin_district \n",
      "\n",
      "city san_francisco location day airbnb head pillow \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in clean_sents[0:5]:\n",
    "    print(f'{\" \".join(t)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(clean_sents)\n",
    "\n",
    "corpus = [id2word.doc2bow(t) for t in clean_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_train5 = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=corpus,\n",
    "                           num_topics=10,\n",
    "                           id2word=id2word)\n",
    "#                            chunksize=100,\n",
    "#                            workers=3, # Num. Processing Cores - 1\n",
    "#                            passes=50,\n",
    "#                            eval_every = 1,\n",
    "#                            per_word_topics=True)\n",
    "    \n",
    "lda_train5.save('lda_train5.model')\n",
    "lda_train5.load('lda_train5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.181*\"stay\" + 0.058*\"city\" + 0.044*\"family\" + 0.037*\"check\" + 0.027*\"way\" '\n",
      "  '+ 0.024*\"friend\" + 0.024*\"time\" + 0.021*\"couple\" + 0.019*\"week\" + '\n",
      "  '0.017*\"san_francisco\"'),\n",
      " (1,\n",
      "  '0.138*\"home\" + 0.043*\"thing\" + 0.037*\"amenity\" + 0.029*\"guest\" + '\n",
      "  '0.017*\"question\" + 0.016*\"hospitality\" + 0.015*\"suggestion\" + 0.012*\"suite\" '\n",
      "  '+ 0.012*\"love\" + 0.011*\"et\"'),\n",
      " (2,\n",
      "  '0.168*\"apartment\" + 0.098*\"space\" + 0.061*\"time\" + 0.057*\"parking\" + '\n",
      "  '0.034*\"street\" + 0.030*\"spot\" + 0.025*\"car\" + 0.024*\"visit\" + 0.019*\"need\" '\n",
      "  '+ 0.016*\"town\"'),\n",
      " (3,\n",
      "  '0.423*\"place\" + 0.064*\"experience\" + 0.050*\"airbnb\" + 0.028*\"san_francisco\" '\n",
      "  '+ 0.022*\"time\" + 0.017*\"san_fran\" + 0.017*\"unit\" + 0.016*\"city\" + '\n",
      "  '0.014*\"dog\" + 0.011*\"photo\"'),\n",
      " (4,\n",
      "  '0.081*\"bed\" + 0.079*\"day\" + 0.048*\"night\" + 0.025*\"lot\" + 0.023*\"person\" + '\n",
      "  '0.021*\"picture\" + 0.019*\"tip\" + 0.018*\"noise\" + 0.012*\"michael\" + '\n",
      "  '0.012*\"sleep\"'),\n",
      " (5,\n",
      "  '0.210*\"location\" + 0.184*\"house\" + 0.053*\"people\" + 0.039*\"studio\" + '\n",
      "  '0.014*\"book\" + 0.013*\"san_francisco\" + 0.012*\"time\" + 0.012*\"listing\" + '\n",
      "  '0.008*\"pleasure\" + 0.008*\"art\"'),\n",
      " (6,\n",
      "  '0.097*\"area\" + 0.056*\"communication\" + 0.017*\"privacy\" + 0.016*\"property\" + '\n",
      "  '0.012*\"staying\" + 0.009*\"son\" + 0.008*\"owner\" + 0.008*\"loft\" + '\n",
      "  '0.008*\"appartment\" + 0.007*\"made_sure\"'),\n",
      " (7,\n",
      "  '0.151*\"room\" + 0.048*\"bathroom\" + 0.047*\"view\" + 0.045*\"kitchen\" + '\n",
      "  '0.025*\"thank\" + 0.020*\"bedroom\" + 0.017*\"morning\" + 0.017*\"shower\" + '\n",
      "  '0.016*\"breakfast\" + 0.016*\"bed\"'),\n",
      " (8,\n",
      "  '0.070*\"restaurant\" + 0.054*\"location\" + 0.031*\"neighborhood\" + '\n",
      "  '0.029*\"downtown\" + 0.023*\"walking_distance\" + 0.023*\"city\" + 0.020*\"shop\" + '\n",
      "  '0.020*\"mission\" + 0.018*\"bar\" + 0.018*\"area\"'),\n",
      " (9,\n",
      "  '0.326*\"host\" + 0.086*\"neighborhood\" + 0.013*\"cat\" + 0.011*\"accommodating\" + '\n",
      "  '0.011*\"travel\" + 0.008*\"question\" + 0.008*\"information\" + 0.006*\"tip\" + '\n",
      "  '0.006*\"towel\" + 0.006*\"service\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_train5.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 0.5248399), (2, 0.27515775), (4, 0.025000982), (0, 0.02500096), (5, 0.025000356), (1, 0.025000053), (9, 0.025000015), (7, 0.025000002), (3, 0.025), (6, 0.025)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = lda_train5.get_document_topics(corpus[0])\n",
    "top_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics_2 = lda_train5.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignTopics(topics):\n",
    "    topic_assign = np.zeros((101793987,10))\n",
    "    for index, topic in enumerate (topics):\n",
    "        for item in topic:\n",
    "            if item[1] >= 0.15:\n",
    "                topic_assign[index][item[0]] +=1\n",
    "    return topic_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix = AssignTopics(top_topics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignTopicsMatrix(topics):\n",
    "    topic_assign = np.zeros((101793987,10))\n",
    "    for index, topic in enumerate (topics):\n",
    "        for item in topic:\n",
    "            topic_assign[index][item[0]] = item[1]\n",
    "    return topic_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_pro_matrix = AssignTopicsMatrix(top_topics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_pro_matrix_df = pd.DataFrame(topic_pro_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101793987, 10)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_pro_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is too larget to pickle, so I split it into two parts and then save them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_pro_matrix_df_part1 = topic_pro_matrix[:50000000]\n",
    "topic_pro_matrix_df_part2 = topic_pro_matrix[50000001:101793987]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('topic_pro_matrix_df_part51.pkl', 'wb') as f:\n",
    "    pickle.dump(topic_pro_matrix_df_part1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('topic_pro_matrix_df_part52.pkl', 'wb') as f:\n",
    "    pickle.dump(topic_pro_matrix_df_part2, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
